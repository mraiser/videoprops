git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp/
nix-shell -p gcc cudatoolkit
make LLAMA_CUBLAS=1
cd models
wget https://huggingface.co/TheBloke/dolphin-2.1-mistral-7B-GGUF/resolve/main/dolphin-2.1-mistral-7b.Q4_0.gguf
cd ../

# EXAMPLE:
# ./main -m models/dolphin-2.1-mistral-7b.Q4_0.gguf -p "Building a website can be done in 10 simple steps:\nStep 1:" -n 50 -e

### Setting the LD_LIBRARY_PATH
./server -m models/dolphin-2.1-mistral-7b.Q4_0.gguf -c 2048
### Modify the following based on the error from above (repeat 1x)
find /nix/store | grep libcublas.so.11
find /nix/store | grep libcudart.so.11.0
### Modify the following based on the result of the above:
export LD_LIBRARY_PATH=/nix/store/da7kq3ibhnyf2vxb1j7pl2wr8w5appih-cudatoolkit-11.8.0/lib/:/nix/store/gnrapfwhq7z2aab8jrdp5pwy0nxc5rwn-cudatoolkit-11.8.0-lib/lib/
./server -m models/dolphin-2.1-mistral-7b.Q4_0.gguf -c 2048

python -m venv venv
pip install flask requests
cd examples/server
python api_like_OAI.py

cd /home/mraiser/ai/chuckme/autogen
nix-shell --run 'source venv/bin/activate; python agengtchat.py'
